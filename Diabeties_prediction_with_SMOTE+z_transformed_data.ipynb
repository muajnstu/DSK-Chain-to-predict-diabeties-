{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muajnstu/Comparative-Analysis-of-K-Nearest-Neighbors-Variants-for-Diabetes-Prediction-Using-Administrative-He/blob/main/diabetics_prediction_with_z_transformed_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diabetics Prediction Model"
      ],
      "metadata": {
        "id": "IS3EoxC_D-i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, roc_auc_score,\n",
        "    f1_score\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import neighbors\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# --- Custom Hybrid KNN+SVM (with memberships) ---\n",
        "class KNNSVM(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=3, plot=True):\n",
        "        self.k = k\n",
        "        self.plot = plot\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.neigh = neighbors.NearestNeighbors(n_neighbors=14)\n",
        "        self.neigh.fit(X, y)\n",
        "        self._check_params(X, y)\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.xdim = len(self.X[0])\n",
        "        self.n = len(y)\n",
        "        self.classes = [0, 1]\n",
        "        self.df = pd.DataFrame(self.X)\n",
        "        self.df['y'] = self.y.values # Use .values to get numpy array for alignment\n",
        "        self.memberships = self._compute_memberships()\n",
        "        self.df['membership'] = self.memberships\n",
        "        self.result = self.neigh.kneighbors(X)\n",
        "        self.label_index = self.result[1]\n",
        "        self.label = []\n",
        "        self.train = []\n",
        "        for i in self.label_index:\n",
        "            for j in i:\n",
        "                one_label = y.iloc[j] # Access using integer position\n",
        "                one_train = X[j]\n",
        "                self.label.append(one_label)\n",
        "                self.train.append(one_train)\n",
        "        self.np_label = np.array(self.label)\n",
        "        self.np_train = np.array(self.train)\n",
        "        self.clf = LinearSVC()\n",
        "        self.clf.fit(self.np_train, self.np_label)\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, r):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict() called before fit()')\n",
        "        if len(set(self.label)) == 1:\n",
        "            return self.label\n",
        "        return self.clf.predict(r)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # LinearSVC does not have predict_proba; use decision_function instead\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict_proba() called before fit()')\n",
        "        if hasattr(self.clf, \"decision_function\"):\n",
        "            decision = self.clf.decision_function(X)\n",
        "            # Normalize to (0,1) range for ROC AUC, shape (n_samples, 2)\n",
        "            if decision.ndim == 1:\n",
        "                min_val, max_val = decision.min(), decision.max()\n",
        "                if min_val == max_val:\n",
        "                    probs = np.ones((len(decision), 2)) * 0.5\n",
        "                else:\n",
        "                    probs = np.zeros((len(decision), 2))\n",
        "                    probs[:, 1] = (decision - min_val) / (max_val - min_val)\n",
        "                    probs[:, 0] = 1 - probs[:, 1]\n",
        "                return probs\n",
        "            else:\n",
        "                # Multiclass\n",
        "                exp_decision = np.exp(decision)\n",
        "                probs = exp_decision / exp_decision.sum(axis=1, keepdims=True)\n",
        "                return probs\n",
        "        else:\n",
        "            n = X.shape[0]\n",
        "            return np.ones((n, 2)) * 0.5\n",
        "\n",
        "    def score(self, X, y):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('score() called before fit()')\n",
        "        predictions = self.predict(X)\n",
        "        predictions = np.asarray(predictions)\n",
        "        return accuracy_score(y_pred=predictions, y_true=y)\n",
        "\n",
        "    def _find_k_nearest_neighbors(self, df, x):\n",
        "        X = df.iloc[:, 0:self.xdim].values\n",
        "        df['distances'] = [np.linalg.norm(X[i] - x) for i in range(self.n)]\n",
        "        df.sort_values(by='distances', ascending=True, inplace=True)\n",
        "        neighbors = df.iloc[0:self.k]\n",
        "        return neighbors\n",
        "\n",
        "    def _get_counts(self, neighbors):\n",
        "        groups = neighbors.groupby('y')\n",
        "        counts = {group[1]['y'].iloc[0]: group[1].count()[0] for group in groups}\n",
        "        return counts\n",
        "\n",
        "    def _compute_memberships(self):\n",
        "        memberships = []\n",
        "        for i in range(self.n):\n",
        "            x = self.X[i]\n",
        "            y = self.y.iloc[i] # Access using integer position\n",
        "            neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "            counts = self._get_counts(neighbors)\n",
        "            membership = dict()\n",
        "            for c in self.classes:\n",
        "                uci = 0.49 * (counts.get(c, 0) / self.k)\n",
        "                if c == y:\n",
        "                    uci += 0.51\n",
        "                membership[c] = uci\n",
        "            memberships.append(membership)\n",
        "        return memberships\n",
        "\n",
        "    def _check_params(self, X, y):\n",
        "        if type(self.k) != int:\n",
        "            raise Exception('\"k\" should have type int')\n",
        "        elif self.k >= len(y):\n",
        "            raise Exception('\"k\" should be less than no of feature sets')\n",
        "        elif self.k % 2 == 0:\n",
        "            raise Exception('\"k\" should be odd')\n",
        "        if type(self.plot) != bool:\n",
        "            raise Exception('\"plot\" should have type bool')\n",
        "\n",
        "\n",
        "# --- Custom Hybrid KNN+Bayes (with memberships) ---\n",
        "class KNNBayes(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=3, plot=True):\n",
        "        self.k = k\n",
        "        self.plot = plot\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.neigh = neighbors.NearestNeighbors(n_neighbors=14)\n",
        "        self.neigh.fit(X, y)\n",
        "        self._check_params(X, y)\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.xdim = len(self.X[0])\n",
        "        self.n = len(y)\n",
        "        self.classes = [0, 1]\n",
        "        self.df = pd.DataFrame(self.X)\n",
        "        self.df['y'] = self.y.values # Use .values to get numpy array for alignment\n",
        "        self.memberships = self._compute_memberships()\n",
        "        self.df['membership'] = self.memberships\n",
        "        self.result = self.neigh.kneighbors(X)\n",
        "        self.label_index = self.result[1]\n",
        "        self.label = []\n",
        "        self.train = []\n",
        "        for i in self.label_index:\n",
        "            for j in i:\n",
        "                one_label = y.iloc[j] # Access using integer position\n",
        "                one_train = X[j]\n",
        "                self.label.append(one_label)\n",
        "                self.train.append(one_train)\n",
        "        self.np_label = np.array(self.label)\n",
        "        self.np_train = np.array(self.train)\n",
        "        self.clf = GaussianNB()\n",
        "        self.clf.fit(self.np_train, self.np_label)\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, r):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict() called before fit()')\n",
        "        if len(set(self.label)) == 1:\n",
        "            return self.label\n",
        "        return self.clf.predict(r)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict_proba() called before fit()')\n",
        "        return self.clf.predict_proba(X)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('score() called before fit()')\n",
        "        predictions = self.predict(X)\n",
        "        predictions = np.asarray(predictions)\n",
        "        return accuracy_score(y_pred=predictions, y_true=y)\n",
        "\n",
        "    def _find_k_nearest_neighbors(self, df, x):\n",
        "        X = df.iloc[:, 0:self.xdim].values\n",
        "        df['distances'] = [np.linalg.norm(X[i] - x) for i in range(self.n)]\n",
        "        df.sort_values(by='distances', ascending=True, inplace=True)\n",
        "        neighbors = df.iloc[0:self.k]\n",
        "        return neighbors\n",
        "\n",
        "    def _get_counts(self, neighbors):\n",
        "        groups = neighbors.groupby('y')\n",
        "        counts = {group[1]['y'].iloc[0]: group[1].count()[0] for group in groups}\n",
        "        return counts\n",
        "\n",
        "    def _compute_memberships(self):\n",
        "        memberships = []\n",
        "        for i in range(self.n):\n",
        "            x = self.X[i]\n",
        "            y = self.y.iloc[i] # Access using integer position\n",
        "            neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "            counts = self._get_counts(neighbors)\n",
        "            membership = dict()\n",
        "            for c in self.classes:\n",
        "                uci = 0.49 * (counts.get(c, 0) / self.k)\n",
        "                if c == y:\n",
        "                    uci += 0.51\n",
        "                membership[c] = uci\n",
        "            memberships.append(membership)\n",
        "        return memberships\n",
        "\n",
        "    def _check_params(self, X, y):\n",
        "        if type(self.k) != int:\n",
        "            raise Exception('\"k\" should have type int')\n",
        "        elif self.k >= len(y):\n",
        "            raise Exception('\"k\" should be less than no of feature sets')\n",
        "        elif self.k % 2 == 0:\n",
        "            raise Exception('\"k\" should be odd')\n",
        "        if type(self.plot) != bool:\n",
        "            raise Exception('\"plot\" should have type bool')\n",
        "\n",
        "\n",
        "class KmeansKNN():\n",
        "    def __init__(self, n_neighbors=3, output='add', n_jobs=None, random_state=0):\n",
        "        self.output = output\n",
        "        self._random_state = random_state\n",
        "        self._cluster = None\n",
        "        self._kclass = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=n_jobs)\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        if type(X_train) != np.ndarray:\n",
        "            X_train = X_train.values\n",
        "        self._cluster = KMeans(n_clusters=len(np.unique(y_train)), random_state=self._random_state).fit(X_train)\n",
        "        y_labels_train = self._cluster.labels_\n",
        "        if self.output == 'add':\n",
        "            X_train = np.append(X_train, np.reshape(y_labels_train, (-1, 1)), axis=1)\n",
        "        elif self.output == 'replace':\n",
        "            X_train = y_labels_train[:, np.newaxis]\n",
        "        else:\n",
        "            raise ValueError('output should be either add or replace')\n",
        "        self._kclass.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        if type(X_test) != np.ndarray:\n",
        "            X_test = X_test.values\n",
        "        y_labels_test = self._cluster.predict(X_test)\n",
        "        if self.output == 'add':\n",
        "            X_test = np.append(X_test, np.reshape(y_labels_test, (-1, 1)), axis=1)\n",
        "        elif self.output == 'replace':\n",
        "            X_test = y_labels_test[:, np.newaxis]\n",
        "        else:\n",
        "            raise ValueError('output should be either add or replace')\n",
        "        return self._kclass.predict(X_test)\n",
        "\n",
        "    def predict_proba(self, X_test):\n",
        "        if type(X_test) != np.ndarray:\n",
        "            X_test = X_test.values\n",
        "        y_labels_test = self._cluster.predict(X_test)\n",
        "        if self.output == 'add':\n",
        "            X_test = np.append(X_test, np.reshape(y_labels_test, (-1, 1)), axis=1)\n",
        "        elif self.output == 'replace':\n",
        "            X_test = y_labels_test[:, np.newaxis]\n",
        "        else:\n",
        "            raise ValueError('output should be either add or replace')\n",
        "        return self._kclass.predict_proba(X_test)\n",
        "\n",
        "# --- Evaluation Metrics ---\n",
        "def print_metrics(y_true, y_pred, y_prob=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.shape == (2, 2):\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        gmean = np.sqrt(sensitivity * specificity)\n",
        "        type1 = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "        type2 = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    else:\n",
        "        specificity = sensitivity = gmean = type1 = type2 = np.nan\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    fmeasure = f1_score(y_true, y_pred, pos_label=1)\n",
        "    auc = None\n",
        "    if y_prob is not None and hasattr(y_prob, \"shape\") and y_prob.shape[1] > 1:\n",
        "        try:\n",
        "            auc = roc_auc_score(y_true, y_prob[:, 1])\n",
        "        except Exception:\n",
        "            auc = 0\n",
        "    else:\n",
        "        auc = 0\n",
        "    print(f\"Accuracy   : {accuracy:.4f}\")\n",
        "    print(f\"Specificity: {specificity:.4f}\")\n",
        "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "    print(f\"G-Mean     : {gmean:.4f}\")\n",
        "    print(f\"F-measure  : {fmeasure:.4f}\")\n",
        "    print(f\"AUC        : {auc:.4f}\")\n",
        "    print(f\"Type-1 error (FPR): {type1:.4f}\")\n",
        "    print(f\"Type-2 error (FNR): {type2:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# --- Data Loading ---\n",
        "data_url = \"https://raw.githubusercontent.com/muajnstu/Comparative-Analysis-of-K-Nearest-Neighbors-Variants-for-Diabetes-Prediction-Using-Administrative-He/refs/heads/main/update_dataframe%20(1).csv\"\n",
        "df = pd.read_csv(data_url)\n",
        "X = df.drop(columns=['Outcome'])\n",
        "y = df['Outcome']\n",
        "print(\"Class distribution:\\n\", y.value_counts())\n",
        "\n",
        "# --- Handle Imbalanced Data ---\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "print(\"Balanced class distribution:\\n\", pd.Series(y_resampled).value_counts())\n",
        "\n",
        "# --- Train/Test Split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=46, stratify=y_resampled)\n",
        "\n",
        "# --- Feature Scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# --- KNN Variant Models ---\n",
        "def run_knn_variant(name, knn_clf):\n",
        "    print(f\"\\n==== {name} ====\")\n",
        "    knn_clf.fit(X_train, y_train)\n",
        "    y_pred = knn_clf.predict(X_test)\n",
        "    if hasattr(knn_clf, \"predict_proba\"):\n",
        "        try:\n",
        "            y_prob = knn_clf.predict_proba(X_test)\n",
        "        except Exception:\n",
        "            y_prob = None\n",
        "    else:\n",
        "        y_prob = None\n",
        "    print_metrics(y_test, y_pred, y_prob)\n",
        "\n",
        "covariance_matrix = np.cov(X_train.T)\n",
        "stabilized_covariance_matrix = covariance_matrix + np.eye(covariance_matrix.shape[0]) * 1e-6\n",
        "inv_covariance_matrix = np.linalg.inv(stabilized_covariance_matrix)\n",
        "\n",
        "knn_variants = {\n",
        "    \"EuclideanKNN\": KNeighborsClassifier(n_neighbors=3, metric='euclidean'),\n",
        "    \"ManhattanKNN\": KNeighborsClassifier(n_neighbors=3, metric='manhattan'),\n",
        "    \"ChebyshevKNN\": KNeighborsClassifier(n_neighbors=3, metric='chebyshev'),\n",
        "    \"MahalanobisKNN\": KNeighborsClassifier(n_neighbors=3, metric='mahalanobis', metric_params={'VI': inv_covariance_matrix}),\n",
        "    \"SeuclideanKNN\": KNeighborsClassifier(n_neighbors=3, metric='seuclidean', metric_params={'V': np.var(X_train, axis=0)}),\n",
        "    \"WminkowskiKNN\": KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=3, metric_params={'w': np.ones(X_train.shape[1])}),\n",
        "    \"DistanceKNN\": KNeighborsClassifier(n_neighbors=3, weights='distance'),\n",
        "    \"GeneralizedKNN\": KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=3),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
        "    \"KNNBayes\": KNNBayes(k=3, plot=False),\n",
        "    \"KNNSVM\": KNNSVM(k=3, plot=False),\n",
        "}\n",
        "\n",
        "for name, model in knn_variants.items():\n",
        "    run_knn_variant(name, model)\n",
        "\n",
        "print(\"\\n==== KMeansKNN ====\")\n",
        "kmeansknn = KmeansKNN(n_neighbors=3, output='add')\n",
        "kmeansknn.fit(X_train, y_train)\n",
        "y_pred = kmeansknn.predict(X_test)\n",
        "y_prob = kmeansknn.predict_proba(X_test)\n",
        "print_metrics(y_test, y_pred, y_prob)\n",
        "\n",
        "print(\"\\nProject complete! Check the above output for performance of each KNN variant.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PysVYKXh5V3D",
        "outputId": "d4dc270b-1559-47e5-94a0-0cd116f67b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution:\n",
            " Outcome\n",
            "0    5405\n",
            "1    1231\n",
            "Name: count, dtype: int64\n",
            "Balanced class distribution:\n",
            " Outcome\n",
            "1    5405\n",
            "0    5405\n",
            "Name: count, dtype: int64\n",
            "\n",
            "==== EuclideanKNN ====\n",
            "Accuracy   : 0.7438\n",
            "Specificity: 0.6466\n",
            "Sensitivity: 0.8409\n",
            "G-Mean     : 0.7374\n",
            "F-measure  : 0.7664\n",
            "AUC        : 0.7973\n",
            "Type-1 error (FPR): 0.3534\n",
            "Type-2 error (FNR): 0.1591\n",
            "Confusion Matrix:\n",
            " [[699 382]\n",
            " [172 909]]\n",
            "\n",
            "==== ManhattanKNN ====\n",
            "Accuracy   : 0.7414\n",
            "Specificity: 0.6392\n",
            "Sensitivity: 0.8437\n",
            "G-Mean     : 0.7344\n",
            "F-measure  : 0.7654\n",
            "AUC        : 0.7952\n",
            "Type-1 error (FPR): 0.3608\n",
            "Type-2 error (FNR): 0.1563\n",
            "Confusion Matrix:\n",
            " [[691 390]\n",
            " [169 912]]\n",
            "\n",
            "==== ChebyshevKNN ====\n",
            "Accuracy   : 0.7155\n",
            "Specificity: 0.6244\n",
            "Sensitivity: 0.8067\n",
            "G-Mean     : 0.7097\n",
            "F-measure  : 0.7393\n",
            "AUC        : 0.7736\n",
            "Type-1 error (FPR): 0.3756\n",
            "Type-2 error (FNR): 0.1933\n",
            "Confusion Matrix:\n",
            " [[675 406]\n",
            " [209 872]]\n",
            "\n",
            "==== MahalanobisKNN ====\n",
            "Accuracy   : 0.7502\n",
            "Specificity: 0.6512\n",
            "Sensitivity: 0.8492\n",
            "G-Mean     : 0.7437\n",
            "F-measure  : 0.7727\n",
            "AUC        : 0.7977\n",
            "Type-1 error (FPR): 0.3488\n",
            "Type-2 error (FNR): 0.1508\n",
            "Confusion Matrix:\n",
            " [[704 377]\n",
            " [163 918]]\n",
            "\n",
            "==== SeuclideanKNN ====\n",
            "Accuracy   : 0.7387\n",
            "Specificity: 0.6457\n",
            "Sensitivity: 0.8316\n",
            "G-Mean     : 0.7328\n",
            "F-measure  : 0.7609\n",
            "AUC        : 0.7959\n",
            "Type-1 error (FPR): 0.3543\n",
            "Type-2 error (FNR): 0.1684\n",
            "Confusion Matrix:\n",
            " [[698 383]\n",
            " [182 899]]\n",
            "\n",
            "==== WminkowskiKNN ====\n",
            "Accuracy   : 0.7396\n",
            "Specificity: 0.6531\n",
            "Sensitivity: 0.8261\n",
            "G-Mean     : 0.7345\n",
            "F-measure  : 0.7603\n",
            "AUC        : 0.7933\n",
            "Type-1 error (FPR): 0.3469\n",
            "Type-2 error (FNR): 0.1739\n",
            "Confusion Matrix:\n",
            " [[706 375]\n",
            " [188 893]]\n",
            "\n",
            "==== DistanceKNN ====\n",
            "Accuracy   : 0.7368\n",
            "Specificity: 0.6392\n",
            "Sensitivity: 0.8344\n",
            "G-Mean     : 0.7303\n",
            "F-measure  : 0.7602\n",
            "AUC        : 0.7945\n",
            "Type-1 error (FPR): 0.3608\n",
            "Type-2 error (FNR): 0.1656\n",
            "Confusion Matrix:\n",
            " [[691 390]\n",
            " [179 902]]\n",
            "\n",
            "==== GeneralizedKNN ====\n",
            "Accuracy   : 0.7396\n",
            "Specificity: 0.6531\n",
            "Sensitivity: 0.8261\n",
            "G-Mean     : 0.7345\n",
            "F-measure  : 0.7603\n",
            "AUC        : 0.7933\n",
            "Type-1 error (FPR): 0.3469\n",
            "Type-2 error (FNR): 0.1739\n",
            "Confusion Matrix:\n",
            " [[706 375]\n",
            " [188 893]]\n",
            "\n",
            "==== KNN ====\n",
            "Accuracy   : 0.7438\n",
            "Specificity: 0.6466\n",
            "Sensitivity: 0.8409\n",
            "G-Mean     : 0.7374\n",
            "F-measure  : 0.7664\n",
            "AUC        : 0.7973\n",
            "Type-1 error (FPR): 0.3534\n",
            "Type-2 error (FNR): 0.1591\n",
            "Confusion Matrix:\n",
            " [[699 382]\n",
            " [172 909]]\n",
            "\n",
            "==== KNNBayes ====\n",
            "Accuracy   : 0.5310\n",
            "Specificity: 0.1526\n",
            "Sensitivity: 0.9093\n",
            "G-Mean     : 0.3726\n",
            "F-measure  : 0.6597\n",
            "AUC        : 0.6587\n",
            "Type-1 error (FPR): 0.8474\n",
            "Type-2 error (FNR): 0.0907\n",
            "Confusion Matrix:\n",
            " [[165 916]\n",
            " [ 98 983]]\n",
            "\n",
            "==== KNNSVM ====\n",
            "Accuracy   : 0.5920\n",
            "Specificity: 0.3654\n",
            "Sensitivity: 0.8187\n",
            "G-Mean     : 0.5469\n",
            "F-measure  : 0.6674\n",
            "AUC        : 0.6555\n",
            "Type-1 error (FPR): 0.6346\n",
            "Type-2 error (FNR): 0.1813\n",
            "Confusion Matrix:\n",
            " [[395 686]\n",
            " [196 885]]\n",
            "\n",
            "==== KMeansKNN ====\n",
            "Accuracy   : 0.7424\n",
            "Specificity: 0.6457\n",
            "Sensitivity: 0.8390\n",
            "G-Mean     : 0.7360\n",
            "F-measure  : 0.7651\n",
            "AUC        : 0.7957\n",
            "Type-1 error (FPR): 0.3543\n",
            "Type-2 error (FNR): 0.1610\n",
            "Confusion Matrix:\n",
            " [[698 383]\n",
            " [174 907]]\n",
            "\n",
            "Project complete! Check the above output for performance of each KNN variant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation"
      ],
      "metadata": {
        "id": "ICzxlqdmDpYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, roc_auc_score,\n",
        "    f1_score\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# --- Wrapper for KmeansKNN for scikit-learn compatibility ---\n",
        "class KmeansKNNWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_neighbors=5, output='add', random_state=0):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.output = output\n",
        "        self.random_state = random_state\n",
        "    def fit(self, X, y):\n",
        "        self.model = KmeansKNN(n_neighbors=self.n_neighbors, output=self.output, random_state=self.random_state)\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "def fold_metrics(y_true, y_pred, y_prob=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.shape == (2, 2):\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        gmean = np.sqrt(sensitivity * specificity)\n",
        "        type1 = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "        type2 = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    else:\n",
        "        specificity = sensitivity = gmean = type1 = type2 = np.nan\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    fmeasure = f1_score(y_true, y_pred, pos_label=1)\n",
        "    auc = 0\n",
        "    if y_prob is not None and hasattr(y_prob, \"shape\") and y_prob.shape[1] > 1:\n",
        "        try:\n",
        "            auc = roc_auc_score(y_true, y_prob[:, 1])\n",
        "        except Exception:\n",
        "            auc = 0\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"specificity\": specificity,\n",
        "        \"sensitivity\": sensitivity,\n",
        "        \"gmean\": gmean,\n",
        "        \"f1_score\": fmeasure,\n",
        "        \"auc\": auc,\n",
        "        \"type1_error\": type1,\n",
        "        \"type2_error\": type2\n",
        "    }\n",
        "\n",
        "def crossval_foldwise_results(models, X, y, n_splits=10, save_path='knn_crossval_results.csv'):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    results = []\n",
        "    for model_name, model in models.items():\n",
        "        print(f'\\n==== {model_name} ====')\n",
        "        for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
        "            X_train, X_test = X[train_idx], X[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "            clf = model\n",
        "            try:\n",
        "                clf.fit(X_train, y_train)\n",
        "                y_pred = clf.predict(X_test)\n",
        "                if hasattr(clf, \"predict_proba\"):\n",
        "                    try:\n",
        "                        y_prob = clf.predict_proba(X_test)\n",
        "                    except Exception:\n",
        "                        y_prob = None\n",
        "                else:\n",
        "                    y_prob = None\n",
        "                metrics = fold_metrics(y_test, y_pred, y_prob)\n",
        "                metrics.update({\n",
        "                    \"model\": model_name,\n",
        "                    \"fold\": fold,\n",
        "                })\n",
        "                results.append(metrics)\n",
        "                print(f\"Fold {fold} : acc={metrics['accuracy']:.4f}, spec={metrics['specificity']:.4f}, sens={metrics['sensitivity']:.4f}, gmean={metrics['gmean']:.4f}, f1={metrics['f1_score']:.4f}, auc={metrics['auc']:.4f}, t1e={metrics['type1_error']:.4f}, t2e={metrics['type2_error']:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in {model_name} fold {fold}: {e}\")\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(save_path, index=False)\n",
        "    print(f\"\\nAll fold results saved to {save_path}\")\n",
        "    return df\n",
        "\n",
        "# --- Prepare data for CV (fit scaler on all resampled data for fair comparison) ---\n",
        "scaler_cv = StandardScaler()\n",
        "X_res_cv = scaler_cv.fit_transform(X_resampled)\n",
        "y_res_cv = np.asarray(y_resampled)\n",
        "\n",
        "# --- Covariance for Mahalanobis ---\n",
        "covariance_matrix_cv = np.cov(X_res_cv.T)\n",
        "stabilized_covariance_matrix_cv = covariance_matrix_cv + np.eye(covariance_matrix_cv.shape[0]) * 1e-6\n",
        "inv_covariance_matrix_cv = np.linalg.inv(stabilized_covariance_matrix_cv)\n",
        "\n",
        "# --- Cross-validation models ---\n",
        "crossval_models = {\n",
        "    \"EuclideanKNN\": KNeighborsClassifier(n_neighbors=5, metric='euclidean'),\n",
        "    \"ManhattanKNN\": KNeighborsClassifier(n_neighbors=5, metric='manhattan'),\n",
        "    \"ChebyshevKNN\": KNeighborsClassifier(n_neighbors=5, metric='chebyshev'),\n",
        "    \"MahalanobisKNN\": KNeighborsClassifier(n_neighbors=5, metric='mahalanobis', metric_params={'VI': inv_covariance_matrix_cv}),\n",
        "    \"SeuclideanKNN\": KNeighborsClassifier(n_neighbors=5, metric='seuclidean', metric_params={'V': np.var(X_res_cv, axis=0)}),\n",
        "    \"WminkowskiKNN\": KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=3, metric_params={'w': np.ones(X_res_cv.shape[1])}),\n",
        "    \"DistanceKNN\": KNeighborsClassifier(n_neighbors=5, weights='distance'),\n",
        "    \"GeneralizedKNN\": KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=3),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"KNNBayes\": KNNBayes(k=5, plot=False),\n",
        "    \"KNNSVM\": KNNSVM(k=5, plot=False),\n",
        "    \"KMeansKNN\": KmeansKNNWrapper(n_neighbors=5, output='add', random_state=0)\n",
        "}\n",
        "\n",
        "# --- Run and save results ---\n",
        "crossval_foldwise_results(crossval_models, X_res_cv, y_res_cv, n_splits=10, save_path='knn_crossval_results.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YCd82NUd8m10",
        "outputId": "514fd763-c3a9-44f6-a076-a01eae6153cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== EuclideanKNN ====\n",
            "Fold 1 : acc=0.7419, spec=0.6519, sens=0.8318, gmean=0.7363, f1=0.7634, auc=0.8077, t1e=0.3481, t2e=0.1682\n",
            "Fold 2 : acc=0.7317, spec=0.6370, sens=0.8262, gmean=0.7255, f1=0.7551, auc=0.7875, t1e=0.3630, t2e=0.1738\n",
            "Fold 3 : acc=0.7465, spec=0.6481, sens=0.8447, gmean=0.7399, f1=0.7694, auc=0.8222, t1e=0.3519, t2e=0.1553\n",
            "Fold 4 : acc=0.7373, spec=0.6593, sens=0.8152, gmean=0.7331, f1=0.7564, auc=0.7996, t1e=0.3407, t2e=0.1848\n",
            "Fold 5 : acc=0.7299, spec=0.6204, sens=0.8392, gmean=0.7215, f1=0.7567, auc=0.7917, t1e=0.3796, t2e=0.1608\n",
            "Fold 6 : acc=0.7308, spec=0.6470, sens=0.8148, gmean=0.7260, f1=0.7515, auc=0.7882, t1e=0.3530, t2e=0.1852\n",
            "Fold 7 : acc=0.7558, spec=0.6691, sens=0.8426, gmean=0.7509, f1=0.7751, auc=0.7998, t1e=0.3309, t2e=0.1574\n",
            "Fold 8 : acc=0.7243, spec=0.6506, sens=0.7981, gmean=0.7206, f1=0.7431, auc=0.7937, t1e=0.3494, t2e=0.2019\n",
            "Fold 9 : acc=0.7345, spec=0.6118, sens=0.8574, gmean=0.7243, f1=0.7634, auc=0.7799, t1e=0.3882, t2e=0.1426\n",
            "Fold 10 : acc=0.7345, spec=0.6377, sens=0.8315, gmean=0.7282, f1=0.7578, auc=0.7892, t1e=0.3623, t2e=0.1685\n",
            "\n",
            "==== ManhattanKNN ====\n",
            "Fold 1 : acc=0.7521, spec=0.6537, sens=0.8503, gmean=0.7455, f1=0.7744, auc=0.8082, t1e=0.3463, t2e=0.1497\n",
            "Fold 2 : acc=0.7354, spec=0.6278, sens=0.8429, gmean=0.7274, f1=0.7613, auc=0.7866, t1e=0.3722, t2e=0.1571\n",
            "Fold 3 : acc=0.7465, spec=0.6426, sens=0.8503, gmean=0.7392, f1=0.7705, auc=0.8233, t1e=0.3574, t2e=0.1497\n",
            "Fold 4 : acc=0.7364, spec=0.6407, sens=0.8318, gmean=0.7300, f1=0.7595, auc=0.8050, t1e=0.3593, t2e=0.1682\n",
            "Fold 5 : acc=0.7280, spec=0.6167, sens=0.8392, gmean=0.7194, f1=0.7554, auc=0.7849, t1e=0.3833, t2e=0.1608\n",
            "Fold 6 : acc=0.7410, spec=0.6470, sens=0.8352, gmean=0.7351, f1=0.7631, auc=0.7932, t1e=0.3530, t2e=0.1648\n",
            "Fold 7 : acc=0.7428, spec=0.6470, sens=0.8389, gmean=0.7367, f1=0.7652, auc=0.7916, t1e=0.3530, t2e=0.1611\n",
            "Fold 8 : acc=0.7373, spec=0.6636, sens=0.8111, gmean=0.7336, f1=0.7552, auc=0.7971, t1e=0.3364, t2e=0.1889\n",
            "Fold 9 : acc=0.7382, spec=0.6137, sens=0.8630, gmean=0.7277, f1=0.7671, auc=0.7899, t1e=0.3863, t2e=0.1370\n",
            "Fold 10 : acc=0.7225, spec=0.6248, sens=0.8204, gmean=0.7159, f1=0.7470, auc=0.7819, t1e=0.3752, t2e=0.1796\n",
            "\n",
            "==== ChebyshevKNN ====\n",
            "Fold 1 : acc=0.7308, spec=0.6463, sens=0.8152, gmean=0.7258, f1=0.7519, auc=0.7815, t1e=0.3537, t2e=0.1848\n",
            "Fold 2 : acc=0.7123, spec=0.6241, sens=0.8004, gmean=0.7067, f1=0.7358, auc=0.7554, t1e=0.3759, t2e=0.1996\n",
            "Fold 3 : acc=0.7336, spec=0.6426, sens=0.8244, gmean=0.7278, f1=0.7559, auc=0.7951, t1e=0.3574, t2e=0.1756\n",
            "Fold 4 : acc=0.7132, spec=0.6370, sens=0.7893, gmean=0.7091, f1=0.7337, auc=0.7728, t1e=0.3630, t2e=0.2107\n",
            "Fold 5 : acc=0.7123, spec=0.6074, sens=0.8170, gmean=0.7045, f1=0.7397, auc=0.7648, t1e=0.3926, t2e=0.1830\n",
            "Fold 6 : acc=0.7095, spec=0.6118, sens=0.8074, gmean=0.7028, f1=0.7352, auc=0.7557, t1e=0.3882, t2e=0.1926\n",
            "Fold 7 : acc=0.7327, spec=0.6303, sens=0.8352, gmean=0.7256, f1=0.7573, auc=0.7808, t1e=0.3697, t2e=0.1648\n",
            "Fold 8 : acc=0.7123, spec=0.6377, sens=0.7870, gmean=0.7084, f1=0.7321, auc=0.7812, t1e=0.3623, t2e=0.2130\n",
            "Fold 9 : acc=0.7058, spec=0.6063, sens=0.8056, gmean=0.6989, f1=0.7323, auc=0.7501, t1e=0.3937, t2e=0.1944\n",
            "Fold 10 : acc=0.6994, spec=0.6174, sens=0.7815, gmean=0.6946, f1=0.7220, auc=0.7425, t1e=0.3826, t2e=0.2185\n",
            "\n",
            "==== MahalanobisKNN ====\n",
            "Fold 1 : acc=0.7475, spec=0.6370, sens=0.8577, gmean=0.7392, f1=0.7727, auc=0.8139, t1e=0.3630, t2e=0.1423\n",
            "Fold 2 : acc=0.7317, spec=0.6259, sens=0.8373, gmean=0.7240, f1=0.7575, auc=0.7925, t1e=0.3741, t2e=0.1627\n",
            "Fold 3 : acc=0.7650, spec=0.6630, sens=0.8669, gmean=0.7581, f1=0.7869, auc=0.8334, t1e=0.3370, t2e=0.1331\n",
            "Fold 4 : acc=0.7475, spec=0.6574, sens=0.8373, gmean=0.7419, f1=0.7684, auc=0.8014, t1e=0.3426, t2e=0.1627\n",
            "Fold 5 : acc=0.7308, spec=0.6222, sens=0.8392, gmean=0.7226, f1=0.7573, auc=0.7959, t1e=0.3778, t2e=0.1608\n",
            "Fold 6 : acc=0.7447, spec=0.6525, sens=0.8370, gmean=0.7390, f1=0.7661, auc=0.8010, t1e=0.3475, t2e=0.1630\n",
            "Fold 7 : acc=0.7456, spec=0.6506, sens=0.8407, gmean=0.7396, f1=0.7675, auc=0.8020, t1e=0.3494, t2e=0.1593\n",
            "Fold 8 : acc=0.7364, spec=0.6599, sens=0.8130, gmean=0.7324, f1=0.7549, auc=0.8022, t1e=0.3401, t2e=0.1870\n",
            "Fold 9 : acc=0.7280, spec=0.6026, sens=0.8537, gmean=0.7172, f1=0.7582, auc=0.7849, t1e=0.3974, t2e=0.1463\n",
            "Fold 10 : acc=0.7243, spec=0.5952, sens=0.8537, gmean=0.7128, f1=0.7557, auc=0.7932, t1e=0.4048, t2e=0.1463\n",
            "\n",
            "==== SeuclideanKNN ====\n",
            "Fold 1 : acc=0.7475, spec=0.6611, sens=0.8336, gmean=0.7424, f1=0.7677, auc=0.8093, t1e=0.3389, t2e=0.1664\n",
            "Fold 2 : acc=0.7308, spec=0.6370, sens=0.8244, gmean=0.7247, f1=0.7540, auc=0.7895, t1e=0.3630, t2e=0.1756\n",
            "Fold 3 : acc=0.7475, spec=0.6500, sens=0.8447, gmean=0.7410, f1=0.7700, auc=0.8227, t1e=0.3500, t2e=0.1553\n",
            "Fold 4 : acc=0.7428, spec=0.6648, sens=0.8207, gmean=0.7387, f1=0.7616, auc=0.8027, t1e=0.3352, t2e=0.1793\n",
            "Fold 5 : acc=0.7262, spec=0.6130, sens=0.8392, gmean=0.7172, f1=0.7542, auc=0.7901, t1e=0.3870, t2e=0.1608\n",
            "Fold 6 : acc=0.7253, spec=0.6396, sens=0.8111, gmean=0.7202, f1=0.7468, auc=0.7832, t1e=0.3604, t2e=0.1889\n",
            "Fold 7 : acc=0.7586, spec=0.6673, sens=0.8500, gmean=0.7531, f1=0.7786, auc=0.8014, t1e=0.3327, t2e=0.1500\n",
            "Fold 8 : acc=0.7253, spec=0.6488, sens=0.8019, gmean=0.7213, f1=0.7446, auc=0.7954, t1e=0.3512, t2e=0.1981\n",
            "Fold 9 : acc=0.7345, spec=0.6137, sens=0.8556, gmean=0.7246, f1=0.7630, auc=0.7787, t1e=0.3863, t2e=0.1444\n",
            "Fold 10 : acc=0.7327, spec=0.6377, sens=0.8278, gmean=0.7266, f1=0.7557, auc=0.7883, t1e=0.3623, t2e=0.1722\n",
            "\n",
            "==== WminkowskiKNN ====\n",
            "Fold 1 : acc=0.7502, spec=0.6759, sens=0.8244, gmean=0.7465, f1=0.7676, auc=0.8066, t1e=0.3241, t2e=0.1756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: SyntaxWarning: Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 : acc=0.7308, spec=0.6296, sens=0.8318, gmean=0.7237, f1=0.7557, auc=0.7857, t1e=0.3704, t2e=0.1682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: SyntaxWarning: Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 : acc=0.7447, spec=0.6463, sens=0.8429, gmean=0.7381, f1=0.7677, auc=0.8181, t1e=0.3537, t2e=0.1571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: SyntaxWarning: Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 : acc=0.7345, spec=0.6537, sens=0.8152, gmean=0.7300, f1=0.7545, auc=0.7951, t1e=0.3463, t2e=0.1848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: SyntaxWarning: Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 : acc=0.7253, spec=0.6111, sens=0.8392, gmean=0.7161, f1=0.7535, auc=0.7883, t1e=0.3889, t2e=0.1608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: SyntaxWarning: Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 6 : acc=0.7188, spec=0.6359, sens=0.8019, gmean=0.7140, f1=0.7402, auc=0.7793, t1e=0.3641, t2e=0.1981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: SyntaxWarning: Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 7 : acc=0.7530, spec=0.6599, sens=0.8463, gmean=0.7473, f1=0.7739, auc=0.7949, t1e=0.3401, t2e=0.1537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: SyntaxWarning: Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 8 : acc=0.7225, spec=0.6470, sens=0.7981, gmean=0.7186, f1=0.7418, auc=0.7919, t1e=0.3530, t2e=0.2019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: SyntaxWarning: Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 9 : acc=0.7290, spec=0.6211, sens=0.8370, gmean=0.7210, f1=0.7552, auc=0.7791, t1e=0.3789, t2e=0.1630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: SyntaxWarning: Parameter p is found in metric_params. The corresponding parameter from __init__ is ignored.\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 10 : acc=0.7234, spec=0.6266, sens=0.8204, gmean=0.7170, f1=0.7477, auc=0.7824, t1e=0.3734, t2e=0.1796\n",
            "\n",
            "==== DistanceKNN ====\n",
            "Fold 1 : acc=0.7539, spec=0.6500, sens=0.8577, gmean=0.7466, f1=0.7772, auc=0.8121, t1e=0.3500, t2e=0.1423\n",
            "Fold 2 : acc=0.7456, spec=0.6352, sens=0.8558, gmean=0.7373, f1=0.7710, auc=0.7949, t1e=0.3648, t2e=0.1442\n",
            "Fold 3 : acc=0.7521, spec=0.6444, sens=0.8595, gmean=0.7443, f1=0.7763, auc=0.8238, t1e=0.3556, t2e=0.1405\n",
            "Fold 4 : acc=0.7475, spec=0.6537, sens=0.8410, gmean=0.7415, f1=0.7692, auc=0.8026, t1e=0.3463, t2e=0.1590\n",
            "Fold 5 : acc=0.7280, spec=0.6167, sens=0.8392, gmean=0.7194, f1=0.7554, auc=0.7942, t1e=0.3833, t2e=0.1608\n",
            "Fold 6 : acc=0.7428, spec=0.6414, sens=0.8444, gmean=0.7360, f1=0.7664, auc=0.7985, t1e=0.3586, t2e=0.1556\n",
            "Fold 7 : acc=0.7576, spec=0.6691, sens=0.8463, gmean=0.7525, f1=0.7772, auc=0.8007, t1e=0.3309, t2e=0.1537\n",
            "Fold 8 : acc=0.7382, spec=0.6580, sens=0.8185, gmean=0.7339, f1=0.7575, auc=0.7986, t1e=0.3420, t2e=0.1815\n",
            "Fold 9 : acc=0.7512, spec=0.6285, sens=0.8741, gmean=0.7412, f1=0.7782, auc=0.7930, t1e=0.3715, t2e=0.1259\n",
            "Fold 10 : acc=0.7364, spec=0.6118, sens=0.8611, gmean=0.7258, f1=0.7654, auc=0.7857, t1e=0.3882, t2e=0.1389\n",
            "\n",
            "==== GeneralizedKNN ====\n",
            "Fold 1 : acc=0.7502, spec=0.6759, sens=0.8244, gmean=0.7465, f1=0.7676, auc=0.8066, t1e=0.3241, t2e=0.1756\n",
            "Fold 2 : acc=0.7308, spec=0.6296, sens=0.8318, gmean=0.7237, f1=0.7557, auc=0.7857, t1e=0.3704, t2e=0.1682\n",
            "Fold 3 : acc=0.7447, spec=0.6463, sens=0.8429, gmean=0.7381, f1=0.7677, auc=0.8181, t1e=0.3537, t2e=0.1571\n",
            "Fold 4 : acc=0.7345, spec=0.6537, sens=0.8152, gmean=0.7300, f1=0.7545, auc=0.7951, t1e=0.3463, t2e=0.1848\n",
            "Fold 5 : acc=0.7253, spec=0.6111, sens=0.8392, gmean=0.7161, f1=0.7535, auc=0.7883, t1e=0.3889, t2e=0.1608\n",
            "Fold 6 : acc=0.7188, spec=0.6359, sens=0.8019, gmean=0.7140, f1=0.7402, auc=0.7793, t1e=0.3641, t2e=0.1981\n",
            "Fold 7 : acc=0.7530, spec=0.6599, sens=0.8463, gmean=0.7473, f1=0.7739, auc=0.7949, t1e=0.3401, t2e=0.1537\n",
            "Fold 8 : acc=0.7225, spec=0.6470, sens=0.7981, gmean=0.7186, f1=0.7418, auc=0.7919, t1e=0.3530, t2e=0.2019\n",
            "Fold 9 : acc=0.7290, spec=0.6211, sens=0.8370, gmean=0.7210, f1=0.7552, auc=0.7791, t1e=0.3789, t2e=0.1630\n",
            "Fold 10 : acc=0.7234, spec=0.6266, sens=0.8204, gmean=0.7170, f1=0.7477, auc=0.7824, t1e=0.3734, t2e=0.1796\n",
            "\n",
            "==== KNN ====\n",
            "Fold 1 : acc=0.7419, spec=0.6519, sens=0.8318, gmean=0.7363, f1=0.7634, auc=0.8077, t1e=0.3481, t2e=0.1682\n",
            "Fold 2 : acc=0.7317, spec=0.6370, sens=0.8262, gmean=0.7255, f1=0.7551, auc=0.7875, t1e=0.3630, t2e=0.1738\n",
            "Fold 3 : acc=0.7465, spec=0.6481, sens=0.8447, gmean=0.7399, f1=0.7694, auc=0.8222, t1e=0.3519, t2e=0.1553\n",
            "Fold 4 : acc=0.7373, spec=0.6593, sens=0.8152, gmean=0.7331, f1=0.7564, auc=0.7996, t1e=0.3407, t2e=0.1848\n",
            "Fold 5 : acc=0.7299, spec=0.6204, sens=0.8392, gmean=0.7215, f1=0.7567, auc=0.7917, t1e=0.3796, t2e=0.1608\n",
            "Fold 6 : acc=0.7308, spec=0.6470, sens=0.8148, gmean=0.7260, f1=0.7515, auc=0.7882, t1e=0.3530, t2e=0.1852\n",
            "Fold 7 : acc=0.7558, spec=0.6691, sens=0.8426, gmean=0.7509, f1=0.7751, auc=0.7998, t1e=0.3309, t2e=0.1574\n",
            "Fold 8 : acc=0.7243, spec=0.6506, sens=0.7981, gmean=0.7206, f1=0.7431, auc=0.7937, t1e=0.3494, t2e=0.2019\n",
            "Fold 9 : acc=0.7345, spec=0.6118, sens=0.8574, gmean=0.7243, f1=0.7634, auc=0.7799, t1e=0.3882, t2e=0.1426\n",
            "Fold 10 : acc=0.7345, spec=0.6377, sens=0.8315, gmean=0.7282, f1=0.7578, auc=0.7892, t1e=0.3623, t2e=0.1685\n",
            "\n",
            "==== KNNBayes ====\n",
            "Error in KNNBayes fold 1: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNBayes fold 2: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNBayes fold 3: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNBayes fold 4: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNBayes fold 5: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNBayes fold 6: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNBayes fold 7: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNBayes fold 8: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNBayes fold 9: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNBayes fold 10: 'numpy.ndarray' object has no attribute 'values'\n",
            "\n",
            "==== KNNSVM ====\n",
            "Error in KNNSVM fold 1: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNSVM fold 2: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNSVM fold 3: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNSVM fold 4: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNSVM fold 5: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNSVM fold 6: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNSVM fold 7: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNSVM fold 8: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNSVM fold 9: 'numpy.ndarray' object has no attribute 'values'\n",
            "Error in KNNSVM fold 10: 'numpy.ndarray' object has no attribute 'values'\n",
            "\n",
            "==== KMeansKNN ====\n",
            "Fold 1 : acc=0.7419, spec=0.6519, sens=0.8318, gmean=0.7363, f1=0.7634, auc=0.8077, t1e=0.3481, t2e=0.1682\n",
            "Fold 2 : acc=0.7317, spec=0.6370, sens=0.8262, gmean=0.7255, f1=0.7551, auc=0.7875, t1e=0.3630, t2e=0.1738\n",
            "Fold 3 : acc=0.7465, spec=0.6481, sens=0.8447, gmean=0.7399, f1=0.7694, auc=0.8212, t1e=0.3519, t2e=0.1553\n",
            "Fold 4 : acc=0.7373, spec=0.6593, sens=0.8152, gmean=0.7331, f1=0.7564, auc=0.7996, t1e=0.3407, t2e=0.1848\n",
            "Fold 5 : acc=0.7299, spec=0.6204, sens=0.8392, gmean=0.7215, f1=0.7567, auc=0.7923, t1e=0.3796, t2e=0.1608\n",
            "Fold 6 : acc=0.7253, spec=0.6396, sens=0.8111, gmean=0.7202, f1=0.7468, auc=0.7835, t1e=0.3604, t2e=0.1889\n",
            "Fold 7 : acc=0.7558, spec=0.6691, sens=0.8426, gmean=0.7509, f1=0.7751, auc=0.7998, t1e=0.3309, t2e=0.1574\n",
            "Fold 8 : acc=0.7243, spec=0.6506, sens=0.7981, gmean=0.7206, f1=0.7431, auc=0.7937, t1e=0.3494, t2e=0.2019\n",
            "Fold 9 : acc=0.7345, spec=0.6118, sens=0.8574, gmean=0.7243, f1=0.7634, auc=0.7799, t1e=0.3882, t2e=0.1426\n",
            "Fold 10 : acc=0.7345, spec=0.6377, sens=0.8315, gmean=0.7282, f1=0.7578, auc=0.7892, t1e=0.3623, t2e=0.1685\n",
            "\n",
            "All fold results saved to knn_crossval_results.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    accuracy  specificity  sensitivity     gmean  f1_score       auc  \\\n",
              "0   0.741906     0.651852     0.831793  0.736346  0.763359  0.807676   \n",
              "1   0.731730     0.637037     0.826248  0.725500  0.755068  0.787530   \n",
              "2   0.746531     0.648148     0.844732  0.739940  0.769360  0.822226   \n",
              "3   0.737280     0.659259     0.815157  0.733076  0.756432  0.799601   \n",
              "4   0.729880     0.620370     0.839187  0.721531  0.756667  0.791739   \n",
              "..       ...          ...          ...       ...       ...       ...   \n",
              "95  0.725254     0.639556     0.811111  0.720244  0.746803  0.783525   \n",
              "96  0.755782     0.669131     0.842593  0.750870  0.775128  0.799810   \n",
              "97  0.724329     0.650647     0.798148  0.720634  0.743103  0.793698   \n",
              "98  0.734505     0.611830     0.857407  0.724284  0.763397  0.779881   \n",
              "99  0.734505     0.637708     0.831481  0.728177  0.757806  0.789199   \n",
              "\n",
              "    type1_error  type2_error         model  fold  \n",
              "0      0.348148     0.168207  EuclideanKNN     1  \n",
              "1      0.362963     0.173752  EuclideanKNN     2  \n",
              "2      0.351852     0.155268  EuclideanKNN     3  \n",
              "3      0.340741     0.184843  EuclideanKNN     4  \n",
              "4      0.379630     0.160813  EuclideanKNN     5  \n",
              "..          ...          ...           ...   ...  \n",
              "95     0.360444     0.188889     KMeansKNN     6  \n",
              "96     0.330869     0.157407     KMeansKNN     7  \n",
              "97     0.349353     0.201852     KMeansKNN     8  \n",
              "98     0.388170     0.142593     KMeansKNN     9  \n",
              "99     0.362292     0.168519     KMeansKNN    10  \n",
              "\n",
              "[100 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0bbd2ceb-f449-4ea9-87a4-1e2caae0a8ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>specificity</th>\n",
              "      <th>sensitivity</th>\n",
              "      <th>gmean</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>auc</th>\n",
              "      <th>type1_error</th>\n",
              "      <th>type2_error</th>\n",
              "      <th>model</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.741906</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.831793</td>\n",
              "      <td>0.736346</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.807676</td>\n",
              "      <td>0.348148</td>\n",
              "      <td>0.168207</td>\n",
              "      <td>EuclideanKNN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.731730</td>\n",
              "      <td>0.637037</td>\n",
              "      <td>0.826248</td>\n",
              "      <td>0.725500</td>\n",
              "      <td>0.755068</td>\n",
              "      <td>0.787530</td>\n",
              "      <td>0.362963</td>\n",
              "      <td>0.173752</td>\n",
              "      <td>EuclideanKNN</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.746531</td>\n",
              "      <td>0.648148</td>\n",
              "      <td>0.844732</td>\n",
              "      <td>0.739940</td>\n",
              "      <td>0.769360</td>\n",
              "      <td>0.822226</td>\n",
              "      <td>0.351852</td>\n",
              "      <td>0.155268</td>\n",
              "      <td>EuclideanKNN</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.737280</td>\n",
              "      <td>0.659259</td>\n",
              "      <td>0.815157</td>\n",
              "      <td>0.733076</td>\n",
              "      <td>0.756432</td>\n",
              "      <td>0.799601</td>\n",
              "      <td>0.340741</td>\n",
              "      <td>0.184843</td>\n",
              "      <td>EuclideanKNN</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.729880</td>\n",
              "      <td>0.620370</td>\n",
              "      <td>0.839187</td>\n",
              "      <td>0.721531</td>\n",
              "      <td>0.756667</td>\n",
              "      <td>0.791739</td>\n",
              "      <td>0.379630</td>\n",
              "      <td>0.160813</td>\n",
              "      <td>EuclideanKNN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.725254</td>\n",
              "      <td>0.639556</td>\n",
              "      <td>0.811111</td>\n",
              "      <td>0.720244</td>\n",
              "      <td>0.746803</td>\n",
              "      <td>0.783525</td>\n",
              "      <td>0.360444</td>\n",
              "      <td>0.188889</td>\n",
              "      <td>KMeansKNN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.755782</td>\n",
              "      <td>0.669131</td>\n",
              "      <td>0.842593</td>\n",
              "      <td>0.750870</td>\n",
              "      <td>0.775128</td>\n",
              "      <td>0.799810</td>\n",
              "      <td>0.330869</td>\n",
              "      <td>0.157407</td>\n",
              "      <td>KMeansKNN</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.724329</td>\n",
              "      <td>0.650647</td>\n",
              "      <td>0.798148</td>\n",
              "      <td>0.720634</td>\n",
              "      <td>0.743103</td>\n",
              "      <td>0.793698</td>\n",
              "      <td>0.349353</td>\n",
              "      <td>0.201852</td>\n",
              "      <td>KMeansKNN</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.734505</td>\n",
              "      <td>0.611830</td>\n",
              "      <td>0.857407</td>\n",
              "      <td>0.724284</td>\n",
              "      <td>0.763397</td>\n",
              "      <td>0.779881</td>\n",
              "      <td>0.388170</td>\n",
              "      <td>0.142593</td>\n",
              "      <td>KMeansKNN</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.734505</td>\n",
              "      <td>0.637708</td>\n",
              "      <td>0.831481</td>\n",
              "      <td>0.728177</td>\n",
              "      <td>0.757806</td>\n",
              "      <td>0.789199</td>\n",
              "      <td>0.362292</td>\n",
              "      <td>0.168519</td>\n",
              "      <td>KMeansKNN</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows  10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0bbd2ceb-f449-4ea9-87a4-1e2caae0a8ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0bbd2ceb-f449-4ea9-87a4-1e2caae0a8ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0bbd2ceb-f449-4ea9-87a4-1e2caae0a8ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-985a0353-b619-44ce-8a8c-4a5ee2dd8532\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-985a0353-b619-44ce-8a8c-4a5ee2dd8532')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-985a0353-b619-44ce-8a8c-4a5ee2dd8532 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"crossval_foldwise_results(crossval_models, X_res_cv, y_res_cv, n_splits=10, save_path='knn_crossval_results\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012447077451109952,\n        \"min\": 0.6993524514338575,\n        \"max\": 0.7650323774283071,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          0.7530064754856615,\n          0.7539315448658649,\n          0.7298797409805735\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"specificity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018365688549725284,\n        \"min\": 0.5951940850277264,\n        \"max\": 0.6759259259259259,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.6407407407407407,\n          0.6759259259259259,\n          0.6025878003696857\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sensitivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.019363701608866205,\n        \"min\": 0.7814814814814814,\n        \"max\": 0.8740740740740741,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          0.8558225508317929,\n          0.8576709796672828,\n          0.8373382624768947\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gmean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012612722312465852,\n        \"min\": 0.6945986684364884,\n        \"max\": 0.758110343693833,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          0.727841231463746,\n          0.7363462444690572,\n          0.7212768509723667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012021749660089693,\n        \"min\": 0.7219846022241232,\n        \"max\": 0.7869127516778524,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          0.7559322033898305,\n          0.7633587786259542,\n          0.7446259673258814\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.015261441604604775,\n        \"min\": 0.7424693640035599,\n        \"max\": 0.8334189087423838,\n        \"num_unique_values\": 73,\n        \"samples\": [\n          0.791738550010269,\n          0.8026305880742111,\n          0.789855891011159\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type1_error\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018365688549725284,\n        \"min\": 0.32407407407407407,\n        \"max\": 0.4048059149722736,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.3592592592592593,\n          0.32407407407407407,\n          0.3974121996303142\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type2_error\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.019363701608866215,\n        \"min\": 0.1259259259259259,\n        \"max\": 0.21851851851851853,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          0.14417744916820702,\n          0.1423290203327172,\n          0.16266173752310537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"KNN\",\n          \"ManhattanKNN\",\n          \"WminkowskiKNN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation of hybrid model"
      ],
      "metadata": {
        "id": "emMpjbAsAlFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, roc_auc_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "# --- Fixed KNNSVM ---\n",
        "class KNNSVM(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=3, plot=True):\n",
        "        self.k = k\n",
        "        self.plot = plot\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        from sklearn import neighbors\n",
        "        from sklearn.svm import LinearSVC\n",
        "        self.neigh = neighbors.NearestNeighbors(n_neighbors=14)\n",
        "        self.neigh.fit(X, y)\n",
        "        self._check_params(X, y)\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.xdim = len(self.X[0])\n",
        "        self.n = len(y)\n",
        "        self.classes = [0, 1]\n",
        "        self.df = pd.DataFrame(self.X)\n",
        "        self.df['y'] = np.array(self.y)  # FIX: supports numpy and pandas\n",
        "        self.memberships = self._compute_memberships()\n",
        "        self.df['membership'] = self.memberships\n",
        "        self.result = self.neigh.kneighbors(X)\n",
        "        self.label_index = self.result[1]\n",
        "        self.label = []\n",
        "        self.train = []\n",
        "        for i in self.label_index:\n",
        "            for j in i:\n",
        "                one_label = np.array(y)[j]  # FIX: supports numpy and pandas\n",
        "                one_train = X[j]\n",
        "                self.label.append(one_label)\n",
        "                self.train.append(one_train)\n",
        "        self.np_label = np.array(self.label)\n",
        "        self.np_train = np.array(self.train)\n",
        "        self.clf = LinearSVC()\n",
        "        self.clf.fit(self.np_train, self.np_label)\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, r):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict() called before fit()')\n",
        "        if len(set(self.label)) == 1:\n",
        "            return self.label\n",
        "        return self.clf.predict(r)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict_proba() called before fit()')\n",
        "        if hasattr(self.clf, \"decision_function\"):\n",
        "            decision = self.clf.decision_function(X)\n",
        "            if decision.ndim == 1:\n",
        "                min_val, max_val = decision.min(), decision.max()\n",
        "                if min_val == max_val:\n",
        "                    probs = np.ones((len(decision), 2)) * 0.5\n",
        "                else:\n",
        "                    probs = np.zeros((len(decision), 2))\n",
        "                    probs[:, 1] = (decision - min_val) / (max_val - min_val)\n",
        "                    probs[:, 0] = 1 - probs[:, 1]\n",
        "                return probs\n",
        "            else:\n",
        "                exp_decision = np.exp(decision)\n",
        "                probs = exp_decision / exp_decision.sum(axis=1, keepdims=True)\n",
        "                return probs\n",
        "        else:\n",
        "            n = X.shape[0]\n",
        "            return np.ones((n, 2)) * 0.5\n",
        "\n",
        "    def score(self, X, y):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('score() called before fit()')\n",
        "        predictions = self.predict(X)\n",
        "        predictions = np.asarray(predictions)\n",
        "        return accuracy_score(y_pred=predictions, y_true=y)\n",
        "\n",
        "    def _find_k_nearest_neighbors(self, df, x):\n",
        "        X = df.iloc[:, 0:self.xdim].values\n",
        "        df['distances'] = [np.linalg.norm(X[i] - x) for i in range(self.n)]\n",
        "        df.sort_values(by='distances', ascending=True, inplace=True)\n",
        "        neighbors = df.iloc[0:self.k]\n",
        "        return neighbors\n",
        "\n",
        "    def _get_counts(self, neighbors):\n",
        "        groups = neighbors.groupby('y')\n",
        "        counts = {group[1]['y'].iloc[0]: group[1].count()[0] for group in groups}\n",
        "        return counts\n",
        "\n",
        "    def _compute_memberships(self):\n",
        "        memberships = []\n",
        "        for i in range(self.n):\n",
        "            x = self.X[i]\n",
        "            y = np.array(self.y)[i]  # FIX: supports numpy and pandas\n",
        "            neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "            counts = self._get_counts(neighbors)\n",
        "            membership = dict()\n",
        "            for c in self.classes:\n",
        "                uci = 0.49 * (counts.get(c, 0) / self.k)\n",
        "                if c == y:\n",
        "                    uci += 0.51\n",
        "                membership[c] = uci\n",
        "            memberships.append(membership)\n",
        "        return memberships\n",
        "\n",
        "    def _check_params(self, X, y):\n",
        "        if type(self.k) != int:\n",
        "            raise Exception('\"k\" should have type int')\n",
        "        elif self.k >= len(y):\n",
        "            raise Exception('\"k\" should be less than no of feature sets')\n",
        "        elif self.k % 2 == 0:\n",
        "            raise Exception('\"k\" should be odd')\n",
        "        if type(self.plot) != bool:\n",
        "            raise Exception('\"plot\" should have type bool')\n",
        "\n",
        "# --- Fixed KNNBayes ---\n",
        "class KNNBayes(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=3, plot=True):\n",
        "        self.k = k\n",
        "        self.plot = plot\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        from sklearn import neighbors\n",
        "        from sklearn.naive_bayes import GaussianNB\n",
        "        self.neigh = neighbors.NearestNeighbors(n_neighbors=14)\n",
        "        self.neigh.fit(X, y)\n",
        "        self._check_params(X, y)\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.xdim = len(self.X[0])\n",
        "        self.n = len(y)\n",
        "        self.classes = [0, 1]\n",
        "        self.df = pd.DataFrame(self.X)\n",
        "        self.df['y'] = np.array(self.y)  # FIX: supports numpy and pandas\n",
        "        self.memberships = self._compute_memberships()\n",
        "        self.df['membership'] = self.memberships\n",
        "        self.result = self.neigh.kneighbors(X)\n",
        "        self.label_index = self.result[1]\n",
        "        self.label = []\n",
        "        self.train = []\n",
        "        for i in self.label_index:\n",
        "            for j in i:\n",
        "                one_label = np.array(y)[j]  # FIX: supports numpy and pandas\n",
        "                one_train = X[j]\n",
        "                self.label.append(one_label)\n",
        "                self.train.append(one_train)\n",
        "        self.np_label = np.array(self.label)\n",
        "        self.np_train = np.array(self.train)\n",
        "        self.clf = GaussianNB()\n",
        "        self.clf.fit(self.np_train, self.np_label)\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, r):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict() called before fit()')\n",
        "        if len(set(self.label)) == 1:\n",
        "            return self.label\n",
        "        return self.clf.predict(r)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict_proba() called before fit()')\n",
        "        return self.clf.predict_proba(X)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('score() called before fit()')\n",
        "        predictions = self.predict(X)\n",
        "        predictions = np.asarray(predictions)\n",
        "        return accuracy_score(y_pred=predictions, y_true=y)\n",
        "\n",
        "    def _find_k_nearest_neighbors(self, df, x):\n",
        "        X = df.iloc[:, 0:self.xdim].values\n",
        "        df['distances'] = [np.linalg.norm(X[i] - x) for i in range(self.n)]\n",
        "        df.sort_values(by='distances', ascending=True, inplace=True)\n",
        "        neighbors = df.iloc[0:self.k]\n",
        "        return neighbors\n",
        "\n",
        "    def _get_counts(self, neighbors):\n",
        "        groups = neighbors.groupby('y')\n",
        "        counts = {group[1]['y'].iloc[0]: group[1].count()[0] for group in groups}\n",
        "        return counts\n",
        "\n",
        "    def _compute_memberships(self):\n",
        "        memberships = []\n",
        "        for i in range(self.n):\n",
        "            x = self.X[i]\n",
        "            y = np.array(self.y)[i]  # FIX: supports numpy and pandas\n",
        "            neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "            counts = self._get_counts(neighbors)\n",
        "            membership = dict()\n",
        "            for c in self.classes:\n",
        "                uci = 0.49 * (counts.get(c, 0) / self.k)\n",
        "                if c == y:\n",
        "                    uci += 0.51\n",
        "                membership[c] = uci\n",
        "            memberships.append(membership)\n",
        "        return memberships\n",
        "\n",
        "    def _check_params(self, X, y):\n",
        "        if type(self.k) != int:\n",
        "            raise Exception('\"k\" should have type int')\n",
        "        elif self.k >= len(y):\n",
        "            raise Exception('\"k\" should be less than no of feature sets')\n",
        "        elif self.k % 2 == 0:\n",
        "            raise Exception('\"k\" should be odd')\n",
        "        if type(self.plot) != bool:\n",
        "            raise Exception('\"plot\" should have type bool')\n",
        "\n",
        "# --- Helper for metrics ---\n",
        "def fold_metrics(y_true, y_pred, y_prob=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.shape == (2, 2):\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        gmean = np.sqrt(sensitivity * specificity)\n",
        "        type1 = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "        type2 = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    else:\n",
        "        specificity = sensitivity = gmean = type1 = type2 = np.nan\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    fmeasure = f1_score(y_true, y_pred, pos_label=1)\n",
        "    auc = 0\n",
        "    if y_prob is not None and hasattr(y_prob, \"shape\") and y_prob.shape[1] > 1:\n",
        "        try:\n",
        "            auc = roc_auc_score(y_true, y_prob[:, 1])\n",
        "        except Exception:\n",
        "            auc = 0\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"specificity\": specificity,\n",
        "        \"sensitivity\": sensitivity,\n",
        "        \"gmean\": gmean,\n",
        "        \"f1_score\": fmeasure,\n",
        "        \"auc\": auc,\n",
        "        \"type1_error\": type1,\n",
        "        \"type2_error\": type2\n",
        "    }\n",
        "\n",
        "# --- 10-fold CV for KNNBayes and KNNSVM, print and save to CSV ---\n",
        "def crossval_foldwise_results(models, X, y, n_splits=10, save_path='knnbayes_knnsvm_crossval.csv'):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    results = []\n",
        "    for model_name, model_class in models.items():\n",
        "        print(f'\\n==== {model_name} ====')\n",
        "        for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
        "            X_train, X_test = X[train_idx], X[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "            clf = model_class\n",
        "            try:\n",
        "                clf.fit(X_train, y_train)\n",
        "                y_pred = clf.predict(X_test)\n",
        "                if hasattr(clf, \"predict_proba\"):\n",
        "                    try:\n",
        "                        y_prob = clf.predict_proba(X_test)\n",
        "                    except Exception:\n",
        "                        y_prob = None\n",
        "                else:\n",
        "                    y_prob = None\n",
        "                metrics = fold_metrics(y_test, y_pred, y_prob)\n",
        "                metrics.update({\n",
        "                    \"model\": model_name,\n",
        "                    \"fold\": fold,\n",
        "                })\n",
        "                results.append(metrics)\n",
        "                print(f\"Fold {fold} : acc={metrics['accuracy']:.4f}, spec={metrics['specificity']:.4f}, sens={metrics['sensitivity']:.4f}, gmean={metrics['gmean']:.4f}, f1={metrics['f1_score']:.4f}, auc={metrics['auc']:.4f}, t1e={metrics['type1_error']:.4f}, t2e={metrics['type2_error']:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in {model_name} fold {fold}: {e}\")\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(save_path, index=False)\n",
        "    print(f\"\\nAll fold results saved to {save_path}\")\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "kmNq8Cii-IVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_cv = StandardScaler()\n",
        "X_res_cv = scaler_cv.fit_transform(X_resampled)\n",
        "y_res_cv = np.asarray(y_resampled)\n",
        "\n",
        "crossval_models = {\n",
        "\"KNNBayes\": KNNBayes(k=5, plot=False)\n",
        "}\n",
        "crossval_foldwise_results(crossval_models, X_res_cv, y_res_cv, n_splits=10, save_path='knnbayes_knnsvm_crossval.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMr95bxl-S42",
        "outputId": "b120c7bd-8553-46ef-8c97-5f19d1fcb845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== KNNBayes ====\n",
            "Fold 1 : acc=0.5365, spec=0.1537, sens=0.9187, gmean=0.3758, f1=0.6649, auc=0.6376, t1e=0.8463, t2e=0.0813\n",
            "Fold 2 : acc=0.5180, spec=0.1167, sens=0.9187, gmean=0.3274, f1=0.6561, auc=0.6307, t1e=0.8833, t2e=0.0813\n",
            "Fold 3 : acc=0.5190, spec=0.1241, sens=0.9131, gmean=0.3366, f1=0.6552, auc=0.6256, t1e=0.8759, t2e=0.0869\n",
            "Fold 4 : acc=0.5282, spec=0.1481, sens=0.9076, gmean=0.3667, f1=0.6582, auc=0.6327, t1e=0.8519, t2e=0.0924\n",
            "Fold 5 : acc=0.5282, spec=0.1352, sens=0.9205, gmean=0.3528, f1=0.6614, auc=0.6325, t1e=0.8648, t2e=0.0795\n",
            "Fold 6 : acc=0.5356, spec=0.1553, sens=0.9167, gmean=0.3773, f1=0.6635, auc=0.6342, t1e=0.8447, t2e=0.0833\n",
            "Fold 7 : acc=0.5217, spec=0.1165, sens=0.9278, gmean=0.3287, f1=0.6596, auc=0.6307, t1e=0.8835, t2e=0.0722\n",
            "Fold 8 : acc=0.5356, spec=0.1479, sens=0.9241, gmean=0.3697, f1=0.6653, auc=0.6284, t1e=0.8521, t2e=0.0759\n",
            "Fold 9 : acc=0.5356, spec=0.1479, sens=0.9241, gmean=0.3697, f1=0.6653, auc=0.6209, t1e=0.8521, t2e=0.0759\n",
            "Fold 10 : acc=0.5217, spec=0.1201, sens=0.9241, gmean=0.3332, f1=0.6587, auc=0.6378, t1e=0.8799, t2e=0.0759\n",
            "\n",
            "==== KNNSVM ====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_cv = StandardScaler()\n",
        "X_res_cv = scaler_cv.fit_transform(X_resampled)\n",
        "y_res_cv = np.asarray(y_resampled)\n",
        "\n",
        "crossval_models = {\n",
        "\"KNNSVM\": KNNSVM(k=5, plot=False)\n",
        "}\n",
        "crossval_foldwise_results(crossval_models, X_res_cv, y_res_cv, n_splits=10, save_path='knnbayes_knnsvm_crossval.csv')"
      ],
      "metadata": {
        "id": "eHv3IiHcAwKG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
